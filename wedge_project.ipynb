{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll work through some of the issues we will identify in class that we'll need to solve for the Wedge project. The key tasks are as follows:\n",
    "\n",
    "1. We need to get our file or files out of the zip file.\n",
    "1. We need to be able to read in the file. \n",
    "1. We need to do a few tests: looking for a header row, checking for delimiters, checking for quotes.\n",
    "1. We need to identify the owner number, which has index `45` in the row.\n",
    "1. We need to find the destination row.\n",
    "1. We need to write out the row. \n",
    "\n",
    "I've built some toy examples for us to play with in class. First, let's get a list of the files. The `os` package has a handy function `listdir` that will help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import io\n",
    "\n",
    "import csv \n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import glob \n",
    "\n",
    "import random \n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "import sqlite3 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_files = os.listdir(\"wedge/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_files = []\n",
    "bad_file_list = []\n",
    "for filename in zip_files:\n",
    "    \n",
    "    if filename.startswith(\"._\"):\n",
    "        bad_file_list.append(filename.strip('._'))\n",
    "    else:\n",
    "        good_files.append(filename)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#header names\n",
    "headers = [\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make new directory to hold extracted files \n",
    "temp_folder_name = \"wedge_clean\"\n",
    "\n",
    "if not os.path.isdir(temp_folder_name): #if folder exisits\n",
    "    os.mkdir(temp_folder_name)          # if not, make it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original code \n",
    "\n",
    "for zip_file in good_files : #files found in the wedge directory from the firts extraction of the large zip file\n",
    "    print(zip_file)\n",
    "    with ZipFile(\"wedge/\" + zip_file, 'r') as my_zip_file:#read the zip files in the directory as my_zip_file\n",
    "    \n",
    "        files_inside = my_zip_file.namelist() #assign new name of files inside \n",
    "        for zipped_file in files_inside:\n",
    "            sniffer = csv.Sniffer()\n",
    "            \n",
    "            \n",
    "            with my_zip_file.open(zipped_file,'r') as input_file:\n",
    "                \n",
    "                clean_file_name = input_file.name.replace('.csv','_clean.csv')#rename new files \n",
    "                \n",
    "                with open(\"wedge_clean/\" + clean_file_name,'w') as outfile:#open the output file name in new folder\n",
    "                    outfile.write(\",\".join(headers) + \"\\n\") #write the files and join the headers to the new outfile\n",
    "                    \n",
    "                    rows_printed = 0\n",
    "                    for idx,line in enumerate(input_file):\n",
    "                        \n",
    "                        file_has_header = False\n",
    "                                                                    \n",
    "                        dialect = sniffer.sniff(line.decode(\"utf-8\"))\n",
    "                        line = line.decode(\"utf-8\").strip().split(dialect.delimiter)#take a bytes object line to a normal string, strip it and then split the string to list\n",
    "                        line=[piece.replace(r'\"','') for piece in line] #use list comprehension to remove the double quotes\n",
    "                        line=[piece.replace(r\"//N\",'') for piece in line]#remove null vaules\n",
    "                        line=[piece.replace(r'NULL','') for piece in line]\n",
    "                        line=[piece.replace(r'\\N','') for piece in line]\n",
    "                        \n",
    "                        if len(line) != 50:\n",
    "                            new_column = (\"\".join(line[5:8])) #if the lines > 50 create a new column of the joined items\n",
    "                            del line[5:8] #delete the old columns that should be joined\n",
    "                            line.insert(5,new_column) #insert the correct column in place of deleted ones\n",
    "                        \n",
    "                        if idx == 0:\n",
    "                            if 'datetime' in line[0]:\n",
    "                                file_has_header = True\n",
    "                                                            \n",
    "                        if file_has_header and idx == 0:\n",
    "                            pass #don't print the line\n",
    "                        else:\n",
    "                            outfile.write(\",\".join(line) + \"\\n\")\n",
    "                            rows_printed += 1\n",
    "                           \n",
    "                        \n",
    "print(f\"Hell yes!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_files = \"/Volumes/EXTERNAL/MSBA/ada/assignments/ada-wedge/wedge_clean/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_wedge = os.listdir(\"wedge_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_files = []\n",
    "bad_file_list = []\n",
    "for filename in clean_wedge:\n",
    "    \n",
    "    if filename.startswith(\"._\"):\n",
    "        bad_file_list.append(filename.strip('._'))\n",
    "    else:\n",
    "        good_files.append(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading of the Clean Files into GBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These first two values will be different on your machine. \n",
    "service_path = \"/Volumes/EXTERNAL/MSBA/ada/assignments/ada-wedge/\"\n",
    "service_file = 'Wedge Project-b683332c35a5.json' # change this to your authentication information  \n",
    "gbq_proj_id = 'affable-operand-291614' # change this to your poroject. \n",
    "gbq_dataset_id = 'wedge_data' # and change this to your data set ID\n",
    "\n",
    "# And this should stay the same. \n",
    "private_key =service_path + service_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your credentials\n",
    "credentials = service_account.Credentials.from_service_account_file(service_path + service_file)\n",
    "\n",
    "# And create a client to talk to GBQ\n",
    "client = bigquery.Client(credentials = credentials, project=gbq_proj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see if a table exist in GBQ\n",
    "def tbl_exists(client, table_ref):\n",
    "    from google.cloud.exceptions import NotFound\n",
    "    try:\n",
    "        client.get_table(table_ref)\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "job_config.schema_update_options = [\n",
    "    bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION # This allows us to modify the table. \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config.schema = [\n",
    "    bigquery.SchemaField(\"datetime\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"register_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"emp_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"upc\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"description\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_subtype\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_status\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"department\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"quantity\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"Scale\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"cost\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"unitPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"total\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"regPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"altPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tax\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"taxexempt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"foodstamp\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"wicable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"memDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discountable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discounttype\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"voided\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"percentDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"ItemQtty\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"volDiscType\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"volume\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"VolSpecial\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"mixMatch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"matched\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"memType\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"staff\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"numflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"itemstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tenderstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"charflag\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"varflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"batchHeaderID\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"local\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"organic\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"display\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"receipt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"card_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"store\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"branch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"match_id\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_id\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "]\n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "job_config.skip_leading_rows = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in good_files:\n",
    "    my_table,junk = file.split(\"_clean\")#splits on _clean in csv files and removes rest as junk\n",
    "    table_full_name = \".\".join([gbq_proj_id,gbq_dataset_id,my_table]) #creates GBQ table name\n",
    "    \n",
    "    if not tbl_exists(client, table_full_name) :\n",
    "        table_ref = client.create_table(table = table_full_name)\n",
    "    else :\n",
    "        table_ref = client.get_table(table_full_name)\n",
    "    \n",
    "    table = client.get_table(table_ref)\n",
    "    print(\"Table {} contains {} columns\".format(table_ref.table_id,len(table.schema)))\n",
    "    \n",
    "    with open(path_to_files + file, \"rb\") as source_file:\n",
    "        job = client.load_table_from_file(\n",
    "            source_file,\n",
    "            table_ref,\n",
    "            location=\"US\",  # Must match the destination dataset location.\n",
    "            job_config=job_config,\n",
    "        )  # API request\n",
    "        \n",
    "    job.result()  # Waits for table load to complete.\n",
    "    print(\"Loaded {} rows into {}:{}.\".format(job.output_rows, 'wedge_example', table_ref.table_id))\n",
    "    \n",
    "    \n",
    "\n",
    "# Checks the updated length of the schema\n",
    "    table = client.get_table(table)\n",
    "    print(\"Table {} now contains {} columns.\".format(table_ref.table_id, len(table.schema)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to remove Tables in Dataset use code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in clean_wedge:\n",
    "    my_table,junk = file.split(\"_clean\")\n",
    "    table_full_name = \".\".join([gbq_proj_id,gbq_dataset_id,my_table]) #creates GBQ table name\n",
    "    query_text =\"\".join(['DROP TABLE `',table_full_name,'`'])\n",
    "# you have to have WHERE clause in a DELETE for GBQ\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(\n",
    "        query_text,\n",
    "        location=\"US\",\n",
    "        job_config=job_config,\n",
    "    )  # API request - starts the query\n",
    "    query_job.result()  # Waits for the query to finish\n",
    "    \n",
    "    print(f'Table {table_full_name} was dropped')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we will connect to the GBQ instance and build a list of owners, take a sample of the owners and extract all the records associated with those owners and write them to a txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths to connect to GBQ\n",
    "\n",
    "service_path = \"/Volumes/EXTERNAL/MSBA/ada/assignments/ada-wedge/\"\n",
    "service_file = 'Wedge Project-b683332c35a5.json' # change this to your authentication information  \n",
    "gbq_proj_id = 'affable-operand-291614' # change this to your poroject. \n",
    "gbq_dataset_id = 'wedge_data' # and change this to your data set ID\n",
    "\n",
    "private_key =service_path + service_file\n",
    "# Get your credentials\n",
    "credentials = service_account.Credentials.from_service_account_file(service_path + service_file)\n",
    "\n",
    "# And create a client to talk to GBQ\n",
    "client = bigquery.Client(credentials = credentials, project=gbq_proj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the query used from GBQ to pull owners\n",
    "query = (\"Select Distinct card_no,\"\n",
    "\"From `affable-operand-291614.wedge_data.transArchive*`\"\n",
    "\"Where card_no !=3\"\n",
    ")\n",
    "\n",
    "#Execute the query using `client.query`\n",
    "\n",
    "query_job = client.query(query,location = \"US\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a list of all the distinct owners from the GBQ tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owners = []\n",
    "\n",
    "for idx, row in enumerate(query_job): \n",
    "    card_no = row[0] #since row in GBQ is a tuple you need to place [] and \n",
    "    \n",
    "    owners.append(card_no)\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the list we will pull a random sample of 325 owners. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20201015)#start random generator at same spot\n",
    "owner_sample = random.choices(owners,k=325) #random list of owners\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will take the query built in GBQ and utilize the random sample of owners to pull the data. There are 2 ways of joinging the query with the list of owners. The cleaner option is run with the list comprehension while another option is commented out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Select * From `affable-operand-291614.wedge_data.transArchive*` Where card_no in (\"\"\"\n",
    "\n",
    "# for owner in owner_sample:\n",
    "#     query += str(owner)+ \",\"\n",
    "    \n",
    "# query = query[:-1] + \")\"\n",
    "\n",
    "\n",
    "\n",
    "query = query + \",\".join([str(owner) for owner in owner_sample]) + \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = client.query(query,location = \"US\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#header names\n",
    "headers = [\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"owner_query.txt\", \"w\") as outfile: \n",
    "    outfile.write(\",\".join(headers) + \"\\n\") #write the files and join the headers to the new outfile \n",
    "    for row in query_job: \n",
    "        outfile.write(\",\".join([str(item) for item in row])+\"\\n\")\n",
    "        \n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we will create a new database, run three different queries from GBQ, write them to txt files, and upload them to the new database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new database named Wedge_Task_3\n",
    "\n",
    "db = sqlite3.connect(\"Wedge_Task_3.db\")\n",
    "cur =db.cursor # place the cursor in the start of the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sales by date by hour query \n",
    "\n",
    "query1 = (\n",
    "    \"\"\"SELECT (EXTRACT(date FROM datetime)) AS Date,\n",
    "    (EXTRACT(hour FROM datetime)) AS Hour,\n",
    "    SUM(total) AS Sales,\n",
    "    COUNT(DISTINCT(Date(datetime) || register_no || emp_no || trans_no)) AS Transactions,\n",
    "    SUM(CASE WHEN(trans_status = 'V' OR trans_status = 'R') THEN -1 ELSE 1 END) as Items\n",
    "    FROM `affable-operand-291614.wedge_data.transArchive*` \n",
    "    WHERE card_no != 3\n",
    "    AND department != 0\n",
    "    AND department != 15\n",
    "    AND trans_status != 'M'\n",
    "    AND trans_status != 'C'\n",
    "    AND trans_status != 'J'\n",
    "    AND (trans_status = ''\n",
    "    OR trans_status = ' '\n",
    "    OR trans_status =  'V'\n",
    "    OR trans_status = 'R')\n",
    "    GROUP BY Date, Hour\n",
    "    ORDER BY Date, Hour\"\"\"\n",
    ")\n",
    "\n",
    "# execute queries with `client.query`\n",
    "results1 = client.query(\n",
    "    query1,\n",
    "    location=\"US\",)\n",
    "\n",
    "#Create output txt file for the first query\n",
    "\n",
    "with open('Sales_by_date_by_hour.txt',\"w\") as outfile:\n",
    "    \n",
    "    for row in results1: \n",
    "        outfile.write(\",\".join([str(item) for item in row])+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# input the results of the first query into the Wedge_Task_3 database as the table Sales_by_date_by_hour\n",
    "input_file1 = \"Sales_by_date_by_hour.txt\"\n",
    "\n",
    "db = sqlite3.connect(\"Wedge_Task_3.db\") # connect to the new database\n",
    "cur = db.cursor()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS Sales_by_Date_by_Hour''') # remove the table if it already exists\n",
    "\n",
    "# create the table in the connected database and set up the schema\n",
    "cur.execute('''CREATE TABLE Sales_by_Date_by_Hour (\n",
    "    Date TIMESTAMP,\n",
    "    Hour TIMESTAMP,\n",
    "    Sales REAL,\n",
    "    Transactions INTEGER,\n",
    "    Items TEXT)''')\n",
    "\n",
    "# input the Sales by Date by Hour text data into the established table in the database\n",
    "with open(input_file1, 'r', encoding = 'Latin-1') as infile:\n",
    "    for idx,line in enumerate(infile.readlines()):\n",
    "        line = line.strip().split(',')\n",
    "        cur.execute('''\n",
    "        INSERT INTO Sales_by_Date_by_Hour (Date, Hour, Sales, Transactions, Items)\n",
    "        VALUES (?,?,?,?,?)''', line) #make sure that the ? match up with the column labels for value insertion\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Query of Wedge data in GBQ - Owner, Year, Month, Sales, Transactions, and Items\n",
    "query2 = (\n",
    "    \"\"\"SELECT card_no As Owner,\n",
    "    (EXTRACT(year FROM datetime)) AS Year,\n",
    "    (EXTRACT(month FROM datetime)) AS Month,\n",
    "    SUM(total) AS Sales,\n",
    "    COUNT(DISTINCT(Date(datetime) || register_no || emp_no || trans_no)) AS Transactions,\n",
    "    SUM(CASE WHEN(trans_status = 'V' OR trans_status = 'R') THEN -1 ELSE 1 END) as Items\n",
    "    FROM `affable-operand-291614.wedge_data.transArchive*` \n",
    "    WHERE card_no != 3\n",
    "    AND department != 0\n",
    "    AND department != 15\n",
    "    AND trans_status != 'M'\n",
    "    AND trans_status != 'C'\n",
    "    AND trans_status != 'J'\n",
    "    AND (trans_status = ''\n",
    "    OR trans_status = ' '\n",
    "    OR trans_status =  'V'\n",
    "    OR trans_status = 'R')\n",
    "    GROUP BY Owner, Year, Month\n",
    "    ORDER BY Owner, Year, Month DESC\"\"\"\n",
    ")\n",
    "\n",
    "# And we execute queries with `client.query`\n",
    "results2 = client.query(\n",
    "    query2,\n",
    "    location=\"US\",\n",
    ")\n",
    "\n",
    "#Create output txt file for the first query\n",
    "\n",
    "with open('Sales_by_Owner_Date.txt',\"w\") as outfile:\n",
    "    \n",
    "    for row in results2: \n",
    "        outfile.write(\",\".join([str(item) for item in row])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the results of the first query into the WedgeTask3 database as the table Sales_by_Owner_by_Date\n",
    "input_file2 = \"Sales_by_Owner_Date.txt\"\n",
    "\n",
    "db = sqlite3.connect(\"Wedge_Task_3.db\") # connect to the WedgeTask 3 database\n",
    "cur = db.cursor()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS Sales_by_Owner_Date''') # remove the table if it already exists\n",
    "\n",
    "# create the table in the connected database and set up the schema\n",
    "cur.execute('''CREATE TABLE Sales_by_Owner_Date (\n",
    "    Owner INTEGER,\n",
    "    Year TIMESTAMP,\n",
    "    Month TIMESTAMP,\n",
    "    Sales REAL,\n",
    "    Transactions INTEGER,\n",
    "    Items TEXT)''')\n",
    "\n",
    "# input the Sales by Owner by Date text data into the established table in the database\n",
    "with open(input_file2, 'r', encoding = 'Latin-1') as infile:\n",
    "    for idx,line in enumerate(infile.readlines()):\n",
    "        line = line.strip().split(',')\n",
    "        cur.execute('''\n",
    "        INSERT INTO Sales_by_Owner_Date (Owner, Year, Month, Sales, Transactions, Items)\n",
    "        VALUES (?,?,?,?,?,?)''', line)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product description by year, month w/columns upc, description, dpt #, dpt name, yr, mo, sales, trans, items\n",
    "query3 = (\n",
    "    \"\"\"SELECT Upc AS UPC,\n",
    "    description AS Product,\n",
    "    a.department AS Department,\n",
    "    b.dept_name AS Dept_Name,\n",
    "    (EXTRACT(year FROM datetime)) AS Year,\n",
    "    (EXTRACT(month FROM datetime)) AS Month,\n",
    "    SUM(total) AS Sales,\n",
    "    COUNT(DISTINCT(Date(datetime) || register_no || emp_no || trans_no)) AS Transactions,\n",
    "    SUM(CASE WHEN(trans_status = 'V' OR trans_status = 'R') THEN -1 ELSE 1 END) as Items\n",
    "    FROM `wedge_data.transArchive*` a\n",
    "    LEFT OUTER JOIN `wedge_data.wedge_department` b\n",
    "    ON a.department = b.department\n",
    "    WHERE card_no != 3\n",
    "    AND a.department != 0\n",
    "    AND a.department != 15\n",
    "    AND trans_status != 'M'\n",
    "    AND trans_status != 'C'\n",
    "    AND trans_status != 'J'\n",
    "    AND (trans_status = ''\n",
    "    OR trans_status = ' '\n",
    "    OR trans_status =  'V'\n",
    "    OR trans_status = 'R')\n",
    "    GROUP BY UPC, Product, Department, Dept_Name, Year, Month\n",
    "    ORDER BY UPC, Product, Department, Dept_Name, Year, Month DESC\"\"\"\n",
    ")\n",
    "\n",
    "# And we execute queries with `client.query`\n",
    "results3 = client.query(\n",
    "    query3,\n",
    "    location=\"US\",)\n",
    "\n",
    "# output the thrid query results as a text file\n",
    "with open('Sales_by_Product_by_Date.txt', 'w') as outfile :\n",
    "    for row in results3: \n",
    "        outfile.write(\",\".join([str(item) for item in row])+\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the results of the first query into the WedgeTask3 database as the table Sales_by_Product_by_Date\n",
    "input_file3 = \"Sales_by_Product_by_Date.txt\"\n",
    "\n",
    "db = sqlite3.connect(\"Wedge_Task_3.db\")# connect to the WedgeTask 3 database\n",
    "cur = db.cursor()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS Sales_by_Product_by_Date''') # remove the table if it already exists\n",
    "\n",
    "# create the table in the connected database and set up the schema\n",
    "cur.execute('''CREATE TABLE Sales_by_Product_by_Date (\n",
    "    UPC STRING,\n",
    "    Description STRING,\n",
    "    Department FLOAT,\n",
    "    Dept_Name STRING,\n",
    "    Year TIMESTAMP,\n",
    "    Month TIMESTAMP,\n",
    "    Sales REAL,\n",
    "    Transactions INTEGER,\n",
    "    Items TEXT)''')\n",
    "\n",
    "# input the Sales by Product by Date text data into the established table in the database\n",
    "with open(input_file3, 'r', encoding = 'Latin-1') as infile:\n",
    "    for idx,line in enumerate(infile.readlines()):\n",
    "        line = line.strip().split(',')\n",
    "        cur.execute('''\n",
    "        INSERT INTO Sales_by_Product_by_Date (UPC, Description, Department, Dept_Name, Year, Month, Sales, Transactions, Items)\n",
    "        VALUES (?,?,?,?,?,?,?,?,?)''', line)\n",
    "db.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
